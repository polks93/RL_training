{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40e02df5-21ea-413a-a2c0-52d66914c649",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "from tqdm import tqdm\n",
    "from my_agents import A2C\n",
    "import gymnasium as gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a3859a51-3a44-4c41-8847-aa5f49c4abdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\paolo\\miniconda3\\envs\\gymenv\\Lib\\site-packages\\gymnasium\\vector\\__init__.py:53: UserWarning: \u001b[33mWARN: `gymnasium.vector.make(...)` is deprecated and will be replaced by `gymnasium.make_vec(...)` in v1.0\u001b[0m\n",
      "  gym.logger.warn(\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Init \"\"\"\n",
    "# Parametri env\n",
    "n_envs = 20 \n",
    "n_updates = 2000\n",
    "n_steps_per_update = 128\n",
    "\n",
    "#  Parametri agente\n",
    "gamma = 0.999\n",
    "lam = 0.95\n",
    "ent_coef = 0.01\n",
    "actor_lr = 0.001    # Learning rate actor\n",
    "critic_lr = 0.005   # Learning rate critic\n",
    "\n",
    "# Genero vettore contente tutti gli env\n",
    "envs = gym.vector.make(\"LunarLander-v2\", num_envs=n_envs, max_episode_steps=600)\n",
    "# envs = gym.make_vec(\"LunarLander-v2\", num_envs=n_envs, max_episode_steps=600)\n",
    "\n",
    "# Init osservazioni e azioni\n",
    "obs_shape = envs.single_observation_space.shape[0]\n",
    "action_shape = envs.single_action_space.n\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "# Init agente\n",
    "agent = A2C(obs_shape, action_shape, device, critic_lr, actor_lr, n_envs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f8e09d-b6a9-4ba7-9ec2-97a96ab762a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|â–Ž                                                                                | 9/2000 [00:03<10:51,  3.06it/s]"
     ]
    }
   ],
   "source": [
    "#  Generazion wrapped env\n",
    "envs_wrapper = gym.wrappers.RecordEpisodeStatistics(envs, deque_size=n_envs*n_updates)\n",
    "\n",
    "critic_losses = []\n",
    "actor_losses = []\n",
    "entropies = []\n",
    "\n",
    "for sample_phase in tqdm(range(n_updates)):\n",
    "    \n",
    "    # Reset liste che contengono l'esperienza di ogni episodio\n",
    "    ep_value_preds      = torch.zeros(n_steps_per_update, n_envs, device=device)\n",
    "    ep_rewards          = torch.zeros(n_steps_per_update, n_envs, device=device)\n",
    "    ep_action_log_probs = torch.zeros(n_steps_per_update, n_envs, device=device)\n",
    "    masks               = torch.zeros(n_steps_per_update, n_envs, device=device)\n",
    "\n",
    "    if sample_phase == 0:\n",
    "        states, info = envs_wrapper.reset(seed=42)\n",
    "\n",
    "    # Eseguo n step su ogni env x ottenere dati\n",
    "    for step in range(n_steps_per_update):\n",
    "\n",
    "        #  L'agente seleziona un'azione in base allo stato corrente\n",
    "        actions, action_log_probs, state_value_preds, entropy = agent.select_action(states)\n",
    "\n",
    "        #  Eseguo l'azione selezionata nell'ambiente\n",
    "        states, rewards, terminated, truncated, infos = envs_wrapper.step(actions.cpu().numpy())\n",
    "\n",
    "\n",
    "        #  Salvo i dati dell'iterazione\n",
    "        ep_value_preds[step] = torch.squeeze(state_value_preds)\n",
    "        ep_rewards[step] = torch.tensor(rewards, device=device)\n",
    "        ep_action_log_probs[step] = action_log_probs\n",
    "\n",
    "        #  Aggiungo una mask che vale 1 se l'episiodio sta ancora runnando e vale 0 altrimenti\n",
    "        masks[step] = torch.tensor([not term for term in terminated])\n",
    "\n",
    "    #  Terminate le iterazioni calcolo le perdite per attore e critico\n",
    "    critic_loss, actor_loss = agent.get_losses(\n",
    "        ep_rewards, \n",
    "        ep_action_log_probs, \n",
    "        ep_value_preds, \n",
    "        entropy, \n",
    "        masks,\n",
    "        gamma,\n",
    "        lam, \n",
    "        ent_coef,\n",
    "        device,\n",
    "    )\n",
    "\n",
    "    #  Update network attore e critico\n",
    "    agent.update_parameters(critic_loss, actor_loss)\n",
    "\n",
    "    #  Salvo perdite di attore e critico e entropia\n",
    "    critic_losses.append(critic_loss.detach().cpu().numpy())    \n",
    "    actor_losses.append(actor_loss.detach().cpu().numpy())\n",
    "    entropies.append(entropy.detach().cpu().numpy())\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25bd340d-288a-4d5f-926a-9be9c71cb5c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" plot the results \"\"\"\n",
    "\n",
    "# %matplotlib inline\n",
    "\n",
    "rolling_length = 20\n",
    "fig, axs = plt.subplots(nrows=2, ncols=2, figsize=(12, 5))\n",
    "fig.suptitle(\n",
    "    f\"Training plots for {agent.__class__.__name__} in the LunarLander-v2 environment \\n \\\n",
    "             (n_envs={n_envs}, n_steps_per_update={n_steps_per_update})\"\n",
    ")\n",
    "\n",
    "# episode return\n",
    "axs[0][0].set_title(\"Episode Returns\")\n",
    "episode_returns_moving_average = (\n",
    "    np.convolve(\n",
    "        np.array(envs_wrapper.return_queue).flatten(),\n",
    "        np.ones(rolling_length),\n",
    "        mode=\"valid\",\n",
    "    )\n",
    "    / rolling_length\n",
    ")\n",
    "axs[0][0].plot(\n",
    "    np.arange(len(episode_returns_moving_average)) / n_envs,\n",
    "    episode_returns_moving_average,\n",
    ")\n",
    "axs[0][0].set_xlabel(\"Number of episodes\")\n",
    "\n",
    "# entropy\n",
    "axs[1][0].set_title(\"Entropy\")\n",
    "entropy_moving_average = (\n",
    "    np.convolve(np.array(entropies), np.ones(rolling_length), mode=\"valid\")\n",
    "    / rolling_length\n",
    ")\n",
    "axs[1][0].plot(entropy_moving_average)\n",
    "axs[1][0].set_xlabel(\"Number of updates\")\n",
    "\n",
    "\n",
    "# critic loss\n",
    "axs[0][1].set_title(\"Critic Loss\")\n",
    "critic_losses_moving_average = (\n",
    "    np.convolve(\n",
    "        np.array(critic_losses).flatten(), np.ones(rolling_length), mode=\"valid\"\n",
    "    )\n",
    "    / rolling_length\n",
    ")\n",
    "axs[0][1].plot(critic_losses_moving_average)\n",
    "axs[0][1].set_xlabel(\"Number of updates\")\n",
    "\n",
    "\n",
    "# actor loss\n",
    "axs[1][1].set_title(\"Actor Loss\")\n",
    "actor_losses_moving_average = (\n",
    "    np.convolve(np.array(actor_losses).flatten(), np.ones(rolling_length), mode=\"valid\")\n",
    "    / rolling_length\n",
    ")\n",
    "axs[1][1].plot(actor_losses_moving_average)\n",
    "axs[1][1].set_xlabel(\"Number of updates\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7374c39d-a5a3-4dff-8ce4-2b258a5a3697",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path di salvataggio\n",
    "actor_weights_path = \"weights/actor_weights.h5\"\n",
    "critic_weights_path = \"weights/critc_weights.h5\"\n",
    "\n",
    "if not os.path.exists(\"weights\"):\n",
    "    os.mkdir(\"weights\")\n",
    "\n",
    "#  Salvataggio dati agente A2C\n",
    "torch.save(agent.actor.state_dict(), actor_weights_path)\n",
    "torch.save(agent.critic.state_dict(), critic_weights_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d4aab2-44df-4697-beed-6910f58f2222",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
