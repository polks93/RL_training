{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc1e2946-4c85-4b40-8a14-18609d73544a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import my_package\n",
    "from my_package import DQN, ReplayBuffer, select_action, optimize_model, eps_decay, soft_update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74db70de-c3a9-4b5f-8684-025e80ba8842",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "gamma = 0.99\n",
    "alpha = 0.0001\n",
    "\n",
    "eps_max = 1.0\n",
    "eps_min = 0.1\n",
    "exploration_fraction = 1.0\n",
    "\n",
    "hidden_layer_dim = 128\n",
    "target_soft_update = True\n",
    "tau = 0.001\n",
    "\n",
    "buffer_size = 100000\n",
    "batch_size = 256\n",
    "max_episodes = 500\n",
    "data_window = 50\n",
    "\n",
    "env_name = 'ShipQuest-v4'\n",
    "Options = {\n",
    "    'generate_random_ship':             True,\n",
    "    'workspace_safe_distance':          2,\n",
    "    'n_actions' :                       3,\n",
    "    'use_lateral_proximity_sensor':     True,\n",
    "    'proximity_sensor_range':           0.5,\n",
    "    'lateral_proximity_sensor_heading': [np.pi/2, -np.pi/2],\n",
    "    'draw_proximity_sensor':            True,\n",
    "    'init_pose':                        None,\n",
    "    'agent_radius':                     0.1,\n",
    "    'frontal_safe_distance':            0.25,\n",
    "    'lateral_safe_distance':            0.25,\n",
    "    'lidar_params':                     {'n_beams': 10, 'max_range': 1.0, 'FoV': np.pi/3},\n",
    "    'draw_lidar':                       False,\n",
    "    'max_steps':                        2000\n",
    "}\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03deaf2b-84a4-448f-ba44-2684b93df0c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(env_name, Options=Options)\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.n\n",
    "\n",
    "\"\"\" Init neural networks \"\"\"\n",
    "policy_net = DQN(\n",
    "    state_dim=state_dim,\n",
    "    action_dim=action_dim,\n",
    "    device=device,\n",
    "    hidden_dim=hidden_layer_dim,\n",
    ")\n",
    "\n",
    "target_net = DQN(\n",
    "    state_dim=state_dim,\n",
    "    action_dim=action_dim,\n",
    "    device=device,\n",
    "    hidden_dim=hidden_layer_dim,\n",
    ")\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()\n",
    "\n",
    "optimizer = optim.Adam(policy_net.parameters(), lr=alpha)\n",
    "\n",
    "\"\"\" Init buffer \"\"\"\n",
    "buffer = ReplayBuffer(capacity=buffer_size)\n",
    "\n",
    "\"\"\" Data collect variables \"\"\"\n",
    "total_rewards = np.zeros(max_episodes)\n",
    "eps_history = np.zeros(max_episodes)\n",
    "len_episodes = np.zeros(max_episodes)\n",
    "# loss_history = []\n",
    "reward_collected_per_ep = np.zeros(max_episodes)\n",
    "coverage_per_ep = np.zeros(max_episodes)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for episode in range(max_episodes):\n",
    "    state, info = env.reset()\n",
    "    done = False\n",
    "    epsilon = eps_decay(episode, max_episodes, eps_min, eps_max, exploration_fraction, 'linear')\n",
    "    eps_history[episode] = epsilon\n",
    "    \n",
    "    while not done:\n",
    "        action = select_action(state, policy_net, epsilon, action_dim)\n",
    "        next_state, reward, terminated, truncated, info = env.step(action)\n",
    "        buffer.push(state, action, reward, next_state, terminated)\n",
    "        done = terminated or truncated\n",
    "\n",
    "        # Ottimizzo policy net e soft update target net\n",
    "        if len(buffer) > batch_size:\n",
    "            optimize_model(policy_net, target_net, buffer, optimizer, batch_size, gamma, debug=False)\n",
    "            if target_soft_update:\n",
    "                soft_update(policy_net, target_net, tau)\n",
    "            # loss_history.append(loss.cpu().detach().numpy())\n",
    "            # loss_history.append(loss.item())\n",
    "\n",
    "        total_rewards[episode] += reward\n",
    "        len_episodes[episode] += 1\n",
    "        state = next_state\n",
    "\n",
    "    coverage_per_ep[episode] = info['coverage']\n",
    "    \n",
    "    if episode % data_window == 0 and episode != 0:\n",
    "        ma_reward = np.mean(total_rewards[episode-data_window:episode])\n",
    "        ma_coverage = np.mean(coverage_per_ep[episode-data_window:episode])\n",
    "        # ma_loss = np.mean(loss_history[-data_window:])\n",
    "        # print(f\"Ep {episode}/{max_episodes}, MA Reward: {ma_reward:.2f}, MA loss: {ma_loss:.4f}, Eps: {epsilon:.2f}\")\n",
    "        print(f\"Ep {episode}/{max_episodes}, MA Reward: {ma_reward:.2f}, MA Coverage: {ma_coverage:.1f} % , Eps: {epsilon:.2f}\")\n",
    "\n",
    "env.close()\n",
    "\n",
    "total_time_seconds = time.time() - start_time\n",
    "hours = int(total_time_seconds // 3600)\n",
    "minutes = int((total_time_seconds % 3600) // 60)\n",
    "seconds = int(total_time_seconds % 60)\n",
    "print(f\"Training finito in: {hours} ore {minutes} minuti e {seconds} secondi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d43e117-2136-49b9-9bdb-b895187dff57",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Plot rewards \"\"\"\n",
    "window_size = 50\n",
    "ma_reward = np.convolve(total_rewards, np.ones(window_size) / window_size, mode='valid')\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(np.arange(len(total_rewards)), total_rewards)\n",
    "plt.plot(np.arange(window_size - 1, len(total_rewards)), ma_reward, color='red', label=f'Moving Average (Window={window_size})', linewidth=2)\n",
    "plt.title('Total Reward and Moving Average Over Episodes')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total Reward')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "\"\"\" Plot coverage \"\"\"\n",
    "window_size = 50\n",
    "ma_coverage = np.convolve(coverage_per_ep, np.ones(window_size) / window_size, mode='valid')\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(np.arange(len(coverage_per_ep)), coverage_per_ep)\n",
    "plt.plot(np.arange(window_size - 1, len(coverage_per_ep)), ma_coverage, color='red', label=f'Moving Average (Window={window_size})', linewidth=2)\n",
    "plt.title('Coverage and Moving Average Over Episodes')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Coverage %')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "\"\"\" Plot episode duration \"\"\"\n",
    "window_size = 50\n",
    "ma_steps = np.convolve(len_episodes, np.ones(window_size) / window_size, mode='valid')\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(np.arange(len(len_episodes)), len_episodes)\n",
    "plt.plot(np.arange(window_size - 1, max_episodes), ma_steps, color='red', label=f'Moving Average (Window={window_size})', linewidth=2)\n",
    "plt.title('Steps per Episode and Moving Average Over Episodes')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total steps')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "415091d4-1548-4f42-ac56-84c7e7d916bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(env_name, Options=Options, render_mode='human')\n",
    "\n",
    "for ep in range(5):\n",
    "    state, info = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    while not done:\n",
    "        action = select_action(state, policy_net, 0, action_dim)\n",
    "        next_state, reward, terminated, truncated, info = env.step(action)\n",
    "        done = terminated or truncated\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "\n",
    "    print('total reward: ' + str(total_reward))\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ff8094-04f6-4037-b652-cd323bdae8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = {\n",
    "    'gamma' : gamma,\n",
    "    'alpha' : alpha,\n",
    "    'eps_max' : eps_max,\n",
    "    'eps_min' : eps_min,\n",
    "    'exploration_fraction' : exploration_fraction,\n",
    "    'state_dim': state_dim,\n",
    "    'action_dim': action_dim,\n",
    "    'hidden_layer_dim' : hidden_layer_dim,\n",
    "    'target_soft_update' : target_soft_update,\n",
    "    'tau' : tau,\n",
    "    'buffer_size' : buffer_size,\n",
    "    'batch_size' : batch_size,\n",
    "    'max_episodes' : max_episodes,\n",
    "    'model_state_dict': policy_net.state_dict(),\n",
    "    'total_rewards': total_rewards,\n",
    "    'len_episodes' : len_episodes,\n",
    "    'coverage_per_ep': coverage_per_ep,\n",
    "    'Options' : Options,\n",
    "    'env_name': env_name\n",
    "}\n",
    "\n",
    "path = \"ShipQuest-v4_Data/model_with_data_\" + env_name + \"_\" + str(max_episodes) + \"_ep.pth\"\n",
    "torch.save(checkpoint, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae35c04-bf80-47fa-8b6d-40bc58545cf2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
