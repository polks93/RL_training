{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b9feba-9618-44e6-81f8-c899c05f196f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "import torch.nn as nn\n",
    "\n",
    "import my_package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c51ba3be-5749-4d92-829d-f6464f5abee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomCallback(BaseCallback):\n",
    "    def __init__(self, total_timesteps: int, print_window: int = 1e4, verbose: int = 0) -> None:\n",
    "        super(CustomCallback, self).__init__(verbose)\n",
    "        self.print_window = print_window\n",
    "        self.losses = []\n",
    "        self.steps_counter = 0\n",
    "        self.episodes_counter = 0\n",
    "        self.rewards_per_ep = []\n",
    "        self.steps_per_ep = []\n",
    "        self.max_steps = total_timesteps\n",
    "        \n",
    "    def _on_step(self) -> bool:\n",
    "\n",
    "        # Salvo loss\n",
    "        if 'train/loss' in self.model.logger.name_to_value:\n",
    "            self.losses.append(self.model.logger.name_to_value['train/loss'])\n",
    "\n",
    "        # Salvo info fine episodio\n",
    "        if self.locals['dones'][0]:\n",
    "            reward = self.locals['infos'][0]['episode']['r']\n",
    "            self.rewards_per_ep.append(reward)\n",
    "            self.episodes_counter += 1\n",
    "            episode_steps = self.locals['infos'][0]['episode']['l']\n",
    "            self.steps_per_ep.append(episode_steps)\n",
    "\n",
    "        if self.steps_counter % self.print_window == 0 and self.steps_counter != 0:\n",
    "            if len(self.rewards_per_ep) > 100:\n",
    "                ma_reward = np.mean(np.array(self.rewards_per_ep[-100:]))\n",
    "            else:\n",
    "                ma_reward = np.mean(np.array(self.rewards_per_ep))\n",
    "            ma_loss = np.mean(np.array(self.losses[-100:]))\n",
    "            print(f'Steps: {self.steps_counter}/{self.max_steps}, MA reward: {ma_reward:.2f}, MA loss: {ma_loss:.2f}')\n",
    "        self.steps_counter += 1\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90bb6772-11a9-46e8-a6e1-e30a11a2edd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Monitor(gym.make('RewardExplorer-v0'))\n",
    "model = DQN(\n",
    "    'MlpPolicy',\n",
    "    env,\n",
    "    learning_rate = 6.3e-4,\n",
    "    batch_size = 128,\n",
    "    buffer_size = 100000,\n",
    "    learning_starts = 0,\n",
    "    gamma = 0.99,\n",
    "    target_update_interval = 250,\n",
    "    train_freq = 4,\n",
    "    gradient_steps = 128,\n",
    "    exploration_fraction = 0.25,\n",
    "    exploration_final_eps = 0.1,\n",
    "    policy_kwargs = dict(net_arch=[256, 256]),\n",
    "    verbose = 0,\n",
    ")\n",
    "total_timesteps = 300000\n",
    "\n",
    "callback = CustomCallback(total_timesteps=total_timesteps, print_window=1000)\n",
    "model.learn(total_timesteps=total_timesteps, callback=callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "394f321a-017b-483b-a519-9531c644a750",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_rewards = np.array(callback.rewards_per_ep)\n",
    "loss_history = np.array(callback.losses)\n",
    "len_episodes = np.array(callback.steps_per_ep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "623653ef-9a43-474c-afd0-f49806542de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Plot rewards \"\"\"\n",
    "window_size = 50\n",
    "ma_reward = np.convolve(total_rewards, np.ones(window_size) / window_size, mode='valid')\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(np.arange(len(total_rewards)), total_rewards)\n",
    "plt.plot(np.arange(window_size - 1, len(total_rewards), ma_reward, color='red', label=f'Moving Average (Window={window_size})', linewidth=2)\n",
    "plt.title('Total Reward and Moving Average Over Episodes')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total Reward')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "\"\"\" Plot loss per step \"\"\"\n",
    "window_size = 1000\n",
    "ma_loss = np.convolve(loss_history, np.ones(window_size) / window_size, mode='valid')\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(np.arange(len(loss_history)), loss_history, label='Loss per Step')\n",
    "plt.plot(np.arange(window_size - 1, len(loss_history)), ma_loss, color='red', label=f'Moving Average (Window={window_size})', linewidth=2)\n",
    "plt.title('Loss per Step')\n",
    "plt.xlabel('Step')\n",
    "plt.ylabel('Loss')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\"\"\" Plot episode duration \"\"\"\n",
    "window_size = 50\n",
    "ma_steps = np.convolve(len_episodes, np.ones(window_size) / window_size, mode='valid')\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(np.arange(len(len_episodes)), len_episodes)\n",
    "plt.plot(np.arange(window_size - 1, max_episodes), ma_steps, color='red', label=f'Moving Average (Window={window_size})', linewidth=2)\n",
    "plt.title('Steps per Episode and Moving Average Over Episodes')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total steps')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13561d19-ee9b-424f-9c14-61d7d8133385",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs, info = env.reset()\n",
    "done = False\n",
    "while not done:\n",
    "    action, _states = model.predict(obs)\n",
    "    obs, rewards, done, truncated, info = env.step(int(action))\n",
    "    env.render()\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
