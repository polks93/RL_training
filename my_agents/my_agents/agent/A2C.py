import numpy as np
import torch
import torch.nn as nn
from torch import optim

""" Definizione classe agente """
class A2C(nn.Module):
    """
    (Synchronous) Advantage Actor-Critic agent class

    Args:
        n_features:  The number of features of the input state.
        n_actions:   The number of actions the agent can take.
        device:      The device to run the computations on (running on a GPU might be quicker for larger Neural Nets,
                     for this code CPU is totally fine).
        critic_lr:   The learning rate for the critic network (should usually be larger than the actor_lr).
        actor_lr:    The learning rate for the actor network.
        n_envs:      The number of environments that run in parallel (on multiple CPUs) to collect experiences.
    """  
    def __init__(self, n_features: int, n_actions: int, device: torch.device, critic_lr: float, actor_lr: float, n_envs: int) -> None:
        super().__init__()
        self.device = device
        self.n_envs = n_envs

        # Rete neurale critico
        critic_layers = [nn.Linear(n_features, 32), nn.ReLU(), nn.Linear(32, 32), nn.ReLU(), nn.Linear(32, 1)]
        self.critic   = nn.Sequential(*critic_layers).to(self.device)
        self.critic_optim = optim.RMSprop(self.critic.parameters(), lr=critic_lr)     # Ottimizzatore
        
        # Rete neurale attore
        actor_layers  = [nn.Linear(n_features, 32), nn.ReLU(), nn.Linear(32, 32), nn.ReLU(), nn.Linear(32, n_actions)]
        self.actor    = nn.Sequential(*actor_layers).to(self.device)
        self.actor_optim = optim.RMSprop(self.actor.parameters(), lr=actor_lr)        # Ottimizzatore

    """ Funzione che esegue uno step in avanti
    
        Input:  - vettore di stato x
        Output: - Tensore [n_envs x 1] contenente la il valore state value function per lo stato x in ogni ambiente
                - Tensore [n_envs x n_actions] contenente la action logits, ovvero la probabilitÃ  di eseguire ogni azione 
                  x ogni ambiente
    """
    def forward(self, x: np.ndarray) -> tuple[torch.Tensor, torch.Tensor]:
        
        x = torch.Tensor(x).to(self.device)
        
        # Il critico mi fornisce la value function dello stato x
        state_values = self.critic(x)

        # L'attore mi fornisce la logits delle varie azioni
        action_logits_vec = self.actor(x)

        return (state_values, action_logits_vec)
        

    """ Funzione che fornisce una tupla di azioni selezionate e la prob logaritimica associata a tali azioni
    
        Input:  - vettore di stato x
        Output: - Tensore con le azioni selezionate
                - Tensore con le log-prob delle azioni
                - Tensore contenente la state value function
                - Tensore contenente l'entropia
        
        """
    def select_action(self, x: np.ndarray) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:

        #  Calcolo state value function e action logits dalla funzione forward
        state_values, action_logits = self.forward(x)
        
        # Calcolo la probabilit`a di eseguire ogni azione
        action_pd = torch.distributions.Categorical(logits=action_logits)

        actions = action_pd.sample()
        action_log_probs = action_pd.log_prob(actions)
        entropy = action_pd.entropy()
        return(actions, action_log_probs, state_values, entropy)


    """ Funzione get_losses
        
        Computes the loss of a minibatch (transitions collected in one sampling phase) for actor and critic
        using Generalized Advantage Estimation (GAE) to compute the advantages

        Inputs:
            - Tensor [n_steps_per_update x n_envs] with the rewards for each time step in the episode
            - Tensor [n_steps_per_update x n_envs] with the log-probs of the actions taken at each time step in the episode
            - Tensor [n_steps_per_update x n_envs] with the state value predictions for each time step in the episode
            - Tensor [n_steps_per_update x n_envs] with the masks for each time step in the episode
            - The discount factor
            - The GAE hyperparameter. (lam=1 corresponds to Monte-Carlo sampling with high variance and no bias,
                                          and lam=0 corresponds to normal TD-Learning that has a low variance but is biased
                                          because the estimates are generated by a Neural Net).
            - The device to run the computations on (e.g. CPU or GPU).

        Outputs:
            - critic_loss: The critic loss for the minibatch.
            - actor_loss: The actor loss for the minibatch.
    """
    def get_losses(
        self,
        rewards: torch.Tensor,
        action_log_probs: torch.Tensor,
        value_preds: torch.Tensor,
        entropy: torch.Tensor,
        masks: torch.Tensor,
        gamma: float,
        lam: float,
        ent_coef: float,
        device: torch.device,
    ) -> tuple[torch.Tensor, torch.Tensor]:

        T = len(rewards)
        advantages = torch.zeros(T, self.n_envs, device=device)

        #  Calcolo degli adavantages usando l' algoritmo gae
        gae = 0.0
        for t in reversed(range(T - 1)):
            td_error = (
                rewards[t] + gamma * masks[t] * value_preds[t + 1] - value_preds[t]
            )
            gae = td_error + gamma * lam * masks[t] * gae
            advantages[t] = gae

        #  Calcolo la perdita del minibatch per critico e attore
        critic_loss = advantages.pow(2).mean()

        # give a bonus for higher entropy to encourage exploration
        actor_loss = (
            -(advantages.detach() * action_log_probs).mean() - ent_coef * entropy.mean()
        )
        return (critic_loss, actor_loss)

    
    """ Funzione che aggiorna i parametri di attore e critico """
    def update_parameters(self, critic_loss: torch.Tensor, actor_loss: torch.Tensor) -> None:
        
        self.critic_optim.zero_grad()
        critic_loss.backward()
        self.critic_optim.step()

        self.actor_optim.zero_grad()
        actor_loss.backward()
        self.actor_optim.step()