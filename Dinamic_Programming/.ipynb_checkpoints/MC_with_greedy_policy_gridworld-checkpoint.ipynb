{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52d2b267-a627-4a85-8e24-1054326ee4e9",
   "metadata": {},
   "source": [
    "### POLICY IMPROVEMENT CON MONTECARLO "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9b7af64f-0af3-46bf-be11-92a9f7c43bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "\n",
    "def map_to_state(grid_position, grid_size)-> int:\n",
    "    return grid_position[0] * grid_size + grid_position[1]   \n",
    "\n",
    "def state_to_map(state, grid_size):\n",
    "    i, j = divmod(state, grid_size)\n",
    "    return [i,j]\n",
    "\n",
    "def action_to_direction(action):\n",
    "    direction = {\n",
    "            0: np.array([1,0]),      # Giu\n",
    "            1: np.array([0, 1]),     # Destra\n",
    "            2: np.array([-1, 0]),    # Su\n",
    "            3: np.array([0, -1]),    # Sinistra\n",
    "        }   \n",
    "    return direction[action]\n",
    "    \n",
    "def find_neighbors_states(cell, grid_size, actions):\n",
    "    neighbors = []\n",
    "    states = []\n",
    "    for a in actions:\n",
    "        next_cell = cell + action_to_direction(a)\n",
    "        if valid_cell(next_cell, grid_size):\n",
    "            neighbors.append(next_cell)\n",
    "        else:\n",
    "            neighbors.append(cell)\n",
    "\n",
    "    for cell in neighbors:\n",
    "        states.append(map_to_state(cell, grid_size))\n",
    "    return states\n",
    "        \n",
    "def valid_cell(cell, grid_size) -> bool:\n",
    "    if cell[0] < 0 or cell[1] < 0:\n",
    "        return False\n",
    "    elif cell[0] < grid_size and cell[1] < grid_size:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def neigh_to_action(curr_cell, neigh_cell):\n",
    "    action = []\n",
    "    if curr_cell != neigh_cell:\n",
    "        \n",
    "        delta_i = curr_cell[0] - neigh_cell[0]\n",
    "        delta_j = curr_cell[1] - neigh_cell[1]\n",
    "\n",
    "        if delta_i == 1:\n",
    "            action = 2\n",
    "        elif delta_i == -1:\n",
    "            action = 0\n",
    "        elif delta_j == 1:\n",
    "            action = 3\n",
    "        elif delta_j == -1:\n",
    "            action = 1\n",
    "            \n",
    "    return action\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4828bb3c-c53a-4566-915c-1eeda4723fa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-6. -5. -4. -3.]\n",
      " [-5. -4. -3. -2.]\n",
      " [-4. -3. -2. -1.]\n",
      " [-3. -2. -1.  0.]]\n",
      "{0: array([0.5, 0.5, 0. , 0. ]), 1: array([0.5, 0.5, 0. , 0. ]), 2: array([0.5, 0.5, 0. , 0. ]), 3: array([1., 0., 0., 0.]), 4: array([0.5, 0.5, 0. , 0. ]), 5: array([0.5, 0.5, 0. , 0. ]), 6: array([0.5, 0.5, 0. , 0. ]), 7: array([1., 0., 0., 0.]), 8: array([0.5, 0.5, 0. , 0. ]), 9: array([0.5, 0.5, 0. , 0. ]), 10: array([0.5, 0.5, 0. , 0. ]), 11: array([1., 0., 0., 0.]), 12: array([0., 1., 0., 0.]), 13: array([0., 1., 0., 0.]), 14: array([0., 1., 0., 0.]), 15: array([0.25, 0.25, 0.25, 0.25])}\n"
     ]
    }
   ],
   "source": [
    "grid_size = 4\n",
    "states = np.array([], dtype=int)\n",
    "\n",
    "# Trasformo tutte le posizioni della griglia in stati\n",
    "for i in range(grid_size):\n",
    "    for j in range(grid_size):\n",
    "        states = np.append(states, map_to_state([i,j], grid_size))\n",
    "\n",
    "V = np.full(states.shape, 0)\n",
    "R = -1.0 * np.ones(len(states))\n",
    "\n",
    "# Elenco possibili azioni\n",
    "actions = [0, 1, 2, 3]\n",
    "\n",
    "# Policy associata a ogni stato\n",
    "Policy = {}\n",
    "for state in states:\n",
    "    Policy[state] = 0.25*np.ones(4)\n",
    "    \n",
    "# Probabilità di transizione associate alle azioni\n",
    "Pa = {}\n",
    "for k in actions:\n",
    "    Pa[k] = np.zeros((len(states), len(states)))\n",
    "\n",
    "    for i in range(grid_size):\n",
    "        for j in range(grid_size):\n",
    "            curr_cell = [i,j]\n",
    "            curr_state = map_to_state(curr_cell, grid_size)\n",
    "            next_cell = curr_cell + action_to_direction(k)\n",
    "\n",
    "            # Se la cella è nella gridmap, la probabilità di spostarmi in quella cella è 1\n",
    "            if valid_cell(next_cell, grid_size):\n",
    "                next_state = map_to_state(next_cell, grid_size)\n",
    "                Pa[k][curr_state, next_state] = 1\n",
    "                \n",
    "            # Se la cella è fuori dalla gridmap, resto nella cella corrente con probabilità 1\n",
    "            else:\n",
    "                Pa[k][curr_state, curr_state] = 1\n",
    "\n",
    "# Imposto la posizione dei goal. \n",
    "# Vengono modellati come stati dai quali non posso uscire e che non danno nessuna reward negativa ad ogni step\n",
    "# goals_pos = [[0,0],[grid_size-1, grid_size-1]]\n",
    "# goals_states = [map_to_state(goals_pos[0], grid_size), map_to_state(goals_pos[1], grid_size)]\n",
    "goals_pos = [grid_size-1, grid_size-1]\n",
    "goals_states = [map_to_state(goals_pos, grid_size)]\n",
    "\n",
    "for state in goals_states:\n",
    "    R[state] = 0\n",
    "    # Quando mi trovo nel goal, ogni azione che eseguo mi lascia nel goal con p = 1\n",
    "    for k in actions:\n",
    "        Pa[k][state,:] = 0\n",
    "        Pa[k][state,state] = 1\n",
    "\n",
    "\n",
    "\n",
    "steps = 10\n",
    "episodes_per_step = 1000\n",
    "for n in range(steps):\n",
    "\n",
    "    # Trasformo il Markov Decision Process in un Markov Reward Process\n",
    "    Rpi = np.full(R.shape, 0)\n",
    "    Ppi = np.zeros((len(states), len(states)))\n",
    "    \n",
    "    for k in actions:\n",
    "        # Vettore delle probabilità di selezionare l'azione k per ogni stato\n",
    "        action_prob = np.array([Policy[state][k] for state in states])\n",
    "    \n",
    "        # Reward associate alla politica pi\n",
    "        Rpi = action_prob * R + Rpi\n",
    "        \n",
    "        # Matrice di transizione associata alla politica Ppi\n",
    "        Ppi = np.dot(np.diag(action_prob), Pa[k]) + Ppi\n",
    "\n",
    "    # Update Value function \n",
    "    for _ in range(episodes_per_step):\n",
    "        V = Rpi + np.dot(Ppi, V)\n",
    "\n",
    "    # Greedy update della policy\n",
    "    for state in states:\n",
    "        cell = state_to_map(state, grid_size)\n",
    "\n",
    "        # Cerco tra le celle vicine quelle con il + alto di V\n",
    "        V_neigh = np.array([])\n",
    "        neighbors_states = find_neighbors_states(cell, grid_size, actions)\n",
    "        \n",
    "        for neighbor in neighbors_states:\n",
    "            V_neigh = np.append(V_neigh, V[neighbor])\n",
    "        V_max = np.max(V_neigh)\n",
    "        index = np.where(V_neigh == V_max)[0]\n",
    "        N = len(index)\n",
    "\n",
    "        # Update della policy per muovermi verso quelle celle\n",
    "        Policy[state] = np.zeros(4)\n",
    "        for i in index:  \n",
    "            new_action = neigh_to_action(cell, state_to_map(neighbors_states[i], grid_size))\n",
    "\n",
    "            # Condizione che si verifica quando mi trovo nella cella goal\n",
    "            if new_action == []:\n",
    "                Policy[state] = np.ones(4) * 0.25\n",
    "                \n",
    "            else:\n",
    "                Policy[state][new_action] = 1/N + Policy[state][new_action]\n",
    "\n",
    "V_matrix = V.reshape((grid_size, grid_size))\n",
    "print(V_matrix)\n",
    "print(Policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f79f88-7b17-49b1-81fc-f51996e4a1d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
